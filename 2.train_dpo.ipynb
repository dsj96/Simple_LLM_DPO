{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aabe23a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S61.57=-46.33--51.50+56.40E'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run common.ipynb\n",
    "\n",
    "tokenizer.decode(tokenizer.get_data(third_number=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e940d0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[ 1,  6,  4,  ...,  0,  0,  0],\n",
       "          [ 1, 15, 13,  ...,  2,  0,  0],\n",
       "          [ 1, 12,  4,  ...,  0,  0,  0],\n",
       "          ...,\n",
       "          [ 1, 15, 13,  ...,  0,  0,  0],\n",
       "          [ 1, 15,  6,  ...,  0,  0,  0],\n",
       "          [ 1, 15,  5,  ...,  5,  5,  2]], device='cuda:0'),\n",
       "  'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 1, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'),\n",
       "  'label': tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
       "          [-100, -100, -100,  ...,    2, -100, -100],\n",
       "          [-100, -100, -100,  ..., -100, -100, -100],\n",
       "          ...,\n",
       "          [-100, -100, -100,  ..., -100, -100, -100],\n",
       "          [-100, -100, -100,  ..., -100, -100, -100],\n",
       "          [-100, -100, -100,  ...,    5,    5,    2]], device='cuda:0')},\n",
       " {'input_ids': tensor([[ 1,  6,  4,  ...,  0,  0,  0],\n",
       "          [ 1, 15, 13,  ...,  0,  0,  0],\n",
       "          [ 1, 12,  4,  ...,  0,  0,  0],\n",
       "          ...,\n",
       "          [ 1, 15, 13,  ...,  0,  0,  0],\n",
       "          [ 1, 15,  6,  ...,  0,  0,  0],\n",
       "          [ 1, 15,  5,  ...,  0,  0,  0]], device='cuda:0'),\n",
       "  'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'),\n",
       "  'label': tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
       "          [-100, -100, -100,  ..., -100, -100, -100],\n",
       "          [-100, -100, -100,  ..., -100, -100, -100],\n",
       "          ...,\n",
       "          [-100, -100, -100,  ..., -100, -100, -100],\n",
       "          [-100, -100, -100,  ..., -100, -100, -100],\n",
       "          [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batch_data():\n",
    "\n",
    "    def pad(data, split, lens):\n",
    "        #做个白板\n",
    "        input_ids = torch.full((len(data), lens),\n",
    "                               tokenizer.encoder['P'],\n",
    "                               device=device)\n",
    "\n",
    "        #往白板里黏贴数据\n",
    "        for i, d in enumerate(data):\n",
    "            input_ids[i, :len(d)] = torch.LongTensor(d)\n",
    "\n",
    "        attention_mask = (input_ids != tokenizer.encoder['P']).long()\n",
    "\n",
    "        #计算label\n",
    "        label = input_ids.clone()\n",
    "        for l, s in zip(label, split):\n",
    "            #问题和pad的位置是-100\n",
    "            l[:s] = -100\n",
    "            l[l == tokenizer.encoder['P']] = -100\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "    #正确的问答\n",
    "    choice = [tokenizer.get_data(third_number=True) for i in range(64)]\n",
    "\n",
    "    #错误的回答简单地定义为空回答就可以了\n",
    "    split = [i.index(tokenizer.encoder['=']) + 1 for i in choice]\n",
    "    reject = [d[:s] for d, s in zip(choice, split)]\n",
    "    reject = [i + [tokenizer.encoder['E']] for i in reject]\n",
    "\n",
    "    #求最大长度\n",
    "    lens = max([len(i) for i in choice])\n",
    "\n",
    "    return pad(choice, split, lens), pad(reject, split, lens)\n",
    "\n",
    "\n",
    "get_batch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b14b5c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/pt2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModelGEN(\n",
       "  (feature): LlamaModel(\n",
       "    (embed_tokens): Embedding(22, 64)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (o_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (up_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (down_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (fc_out): Linear(in_features=64, out_features=22, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dpo = torch.load('gen.model')\n",
    "model_dpo.to(device)\n",
    "model_dpo.train()\n",
    "\n",
    "model_dpo_ref = torch.load('gen.model')\n",
    "model_dpo_ref.to(device)\n",
    "model_dpo_ref.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c35e3b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-42.1512, -32.2378, -33.3335, -35.5115, -43.3339, -35.5948, -45.4445,\n",
       "        -41.2210, -35.1233, -34.2614, -37.2273, -35.6421, -44.1526, -36.7085,\n",
       "        -39.9655, -35.2811, -34.6775, -39.8615, -39.3645, -37.0570, -31.3551,\n",
       "        -40.6914, -37.8081, -33.7670, -33.3802, -37.0586, -33.6158, -36.1201,\n",
       "        -35.0879, -37.0642, -39.6770, -42.1228, -39.5020, -41.2665, -38.3211,\n",
       "        -37.2448, -38.0346, -37.7231, -53.9730, -37.9990, -37.8209, -43.6167,\n",
       "        -39.8550, -37.9038, -42.0052, -34.4402, -39.1546, -32.8980, -33.8170,\n",
       "        -39.4172, -50.1610, -36.9279, -52.5554, -48.7900, -35.8651, -47.3491,\n",
       "        -34.5138, -34.8623, -40.0592, -36.6584, -37.5624, -39.2018, -38.0018,\n",
       "        -37.3401], device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_prob_log(model, choice, reject):\n",
    "    b = choice['input_ids'].shape[0]\n",
    "\n",
    "    #合并两部分输入,同时计算以提高效率\n",
    "    #[b, 21]\n",
    "    input_ids = torch.cat([choice['input_ids'], reject['input_ids']], dim=0)\n",
    "    attention_mask = torch.cat(\n",
    "        [choice['attention_mask'], reject['attention_mask']], dim=0)\n",
    "    label = torch.cat([choice['label'], reject['label']], dim=0)\n",
    "\n",
    "    #[b, 21, 28]\n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    #偏移以对齐\n",
    "    #[b, 20]\n",
    "    label = label[:, 1:]\n",
    "    #[b, 20, 28]\n",
    "    out = out[:, :-1]\n",
    "\n",
    "    #取所有字的预测概率,因为要求联合概率,所以取对数\n",
    "    out = (out.softmax(2) + 1e-8).log()\n",
    "\n",
    "    #取预测到label的概率\n",
    "    #索引不能是负数,所以这里把负数置0\n",
    "    index = label.clone().unsqueeze(2)\n",
    "    index[index == -100] = 0\n",
    "    prob = out.gather(2, index=index).squeeze(2)\n",
    "\n",
    "    #只取答案部分的loss,筛选后,所有答案的概率对数求和\n",
    "    prob = (prob * (label != -100)).sum(1)\n",
    "\n",
    "    #choice和reject的预测概率求差\n",
    "    return prob[:b] - prob[b:]\n",
    "\n",
    "\n",
    "get_prob_log(model_dpo, *get_batch_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb049323",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 S38.30=77.53+1S.482E\n",
      "1000 S108.85=-29.46--37.05+30.01E\n",
      "2000 S44.90=-4.14/75.14+68.99E\n",
      "3000 S-26.53=10.80/41.26+-49.15E\n",
      "4000 S-36.64=-55.47/-89.69+-42.73E\n",
      "5000 S5.54=55.63/-11.10+10.59E\n",
      "6000 S-73.71=-11.88+-72.64+-20.22E\n",
      "7000 S-36.85=61.31+-77.72+-34.73E\n",
      "8000 S89.76=-22.73/-83.62+88.44E\n",
      "9000 S-28.79=25.78/-87.84+-27.99E\n",
      "10000 S-61.36=-90.41+-5.14+29.17E\n",
      "11000 S-18.05=3.20/15.27+-10.82E\n",
      "12000 S-6.66=32.52/-0.61+4.44E\n",
      "13000 S6060.81=94.70*66.67+-66.46E\n",
      "14000 S-57.69=-56.26/0.30+-37.07E\n",
      "15000 S2590.33=-55.67*-39.12+-5.68E\n",
      "16000 S-210.33=-93.29-25.44+-97.53E\n",
      "17000 S24.79=84.88+36.65+-79.89E\n",
      "18000 S-89.55=-86.22/84.67+-88.10E\n",
      "19000 S89.63=69.98--21.02+1.25E\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model_dpo.parameters(),\n",
    "                             lr=1e-4,\n",
    "                             betas=(0.9, 0.999),\n",
    "                             eps=1e-8)\n",
    "\n",
    "for i in range(2_0000):\n",
    "    choice, reject = get_batch_data()\n",
    "\n",
    "    #两个模型分别计算概率对数\n",
    "    prob_log = get_prob_log(model_dpo, choice, reject)\n",
    "    with torch.no_grad():\n",
    "        prob_log_ref = get_prob_log(model_dpo_ref, choice, reject)\n",
    "\n",
    "    #两份概率计算kl散度\n",
    "    kl = -0.1 * (prob_log - prob_log_ref)\n",
    "\n",
    "    #以kl散度计算loss\n",
    "    loss = (kl.sigmoid() + 1e-8).log().mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        question = tokenizer.get_data(third_number=True)\n",
    "        question = question[:question.index(tokenizer.encoder['=']) + 1]\n",
    "        question = torch.LongTensor(question).unsqueeze(0).to(device)\n",
    "\n",
    "        gen = generate(model_dpo, question)\n",
    "        print(i, tokenizer.decode(gen[0].tolist()))\n",
    "\n",
    "model_dpo.to('cpu')\n",
    "torch.save(model_dpo, 'dpo.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pt2]",
   "language": "python",
   "name": "conda-env-pt2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
