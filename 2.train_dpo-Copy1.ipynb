{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aabe23a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/llama/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'S-92.12=-24.36/91.91+-91.85E'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run common.ipynb\n",
    "\n",
    "tokenizer.decode(tokenizer.get_data(third_number=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c0fd857",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[ 1,  7, 11,  ...,  0,  0,  0],\n",
       "          [ 1, 15, 11,  ...,  2,  0,  0],\n",
       "          [ 1, 15, 11,  ...,  2,  0,  0],\n",
       "          ...,\n",
       "          [ 1, 15,  5,  ..., 12,  2,  0],\n",
       "          [ 1, 15, 12,  ..., 11,  2,  0],\n",
       "          [ 1, 15, 12,  ...,  0,  0,  0]], device='cuda:0'),\n",
       "  'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 1, 0, 0],\n",
       "          [1, 1, 1,  ..., 1, 0, 0],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 1, 1, 0],\n",
       "          [1, 1, 1,  ..., 1, 1, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'),\n",
       "  'label': tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
       "          [-100, -100, -100,  ...,    2, -100, -100],\n",
       "          [-100, -100, -100,  ...,    2, -100, -100],\n",
       "          ...,\n",
       "          [-100, -100, -100,  ...,   12,    2, -100],\n",
       "          [-100, -100, -100,  ...,   11,    2, -100],\n",
       "          [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')},\n",
       " {'input_ids': tensor([[ 1,  7, 11,  ...,  0,  0,  0],\n",
       "          [ 1, 15, 11,  ...,  0,  0,  0],\n",
       "          [ 1, 15, 11,  ...,  0,  0,  0],\n",
       "          ...,\n",
       "          [ 1, 15,  5,  ...,  0,  0,  0],\n",
       "          [ 1, 15, 12,  ...,  0,  0,  0],\n",
       "          [ 1, 15, 12,  ...,  0,  0,  0]], device='cuda:0'),\n",
       "  'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'),\n",
       "  'label': tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
       "          [-100, -100, -100,  ..., -100, -100, -100],\n",
       "          [-100, -100, -100,  ..., -100, -100, -100],\n",
       "          ...,\n",
       "          [-100, -100, -100,  ..., -100, -100, -100],\n",
       "          [-100, -100, -100,  ..., -100, -100, -100],\n",
       "          [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batch_data():\n",
    "\n",
    "    def pad(data, split, lens):\n",
    "        #做个白板\n",
    "        input_ids = torch.full((len(data), lens),\n",
    "                               tokenizer.encoder['P'],\n",
    "                               device=device)\n",
    "\n",
    "        #往白板里黏贴数据\n",
    "        for i, d in enumerate(data):\n",
    "            input_ids[i, :len(d)] = torch.LongTensor(d)\n",
    "\n",
    "        attention_mask = (input_ids != tokenizer.encoder['P']).long()\n",
    "\n",
    "        #计算label\n",
    "        label = input_ids.clone()\n",
    "        for l, s in zip(label, split):\n",
    "            #问题和pad的位置是-100\n",
    "            l[:s] = -100\n",
    "            l[l == tokenizer.encoder['P']] = -100\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "    #正确的问答\n",
    "    choice = [tokenizer.get_data(third_number=True) for i in range(64)]\n",
    "\n",
    "    #错误的回答简单地定义为空回答就可以了\n",
    "    split = [i.index(tokenizer.encoder['=']) + 1 for i in choice]\n",
    "    reject = [d[:s] for d, s in zip(choice, split)]\n",
    "    reject = [i + [tokenizer.encoder['E']] for i in reject]\n",
    "\n",
    "    #求最大长度\n",
    "    lens = max([len(i) for i in choice])\n",
    "\n",
    "    return pad(choice, split, lens), pad(reject, split, lens)\n",
    "\n",
    "\n",
    "get_batch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9099834e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelGEN(\n",
       "  (feature): LlamaModel(\n",
       "    (embed_tokens): Embedding(22, 64, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (o_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (up_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (down_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (fc_out): Linear(in_features=64, out_features=22, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gen = torch.load('gen.model')\n",
    "model_gen.to(device)\n",
    "model_gen.train()\n",
    "\n",
    "model_gen_ref = torch.load('gen.model')\n",
    "model_gen_ref.to(device)\n",
    "model_gen_ref.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fba3a045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-81.4473, -54.0714, -70.3097, -55.9786, -66.7143, -74.3108, -52.3986,\n",
       "        -70.3757, -77.1062, -42.0191, -50.4737, -53.8479, -44.9187, -42.3186,\n",
       "        -72.4670, -48.3093, -86.0694, -76.4324, -55.6429, -59.1275, -78.3820,\n",
       "        -50.7187, -69.7550, -95.5008, -77.0167, -50.7966, -47.0271, -54.3163,\n",
       "        -81.2113, -65.7755, -55.3272, -56.4524, -64.8713, -79.5861, -56.2123,\n",
       "        -78.0945, -73.5479, -39.5135, -41.1529, -38.9173, -74.6226, -67.9325,\n",
       "        -72.8875, -36.5037, -51.1867, -68.7703, -57.0807, -58.2965, -69.4282,\n",
       "        -60.8845, -53.3348, -52.7771, -65.3301, -52.3360, -76.4893, -69.4408,\n",
       "        -46.7414, -87.0744, -62.7352, -55.8983, -74.1987, -43.9923, -38.6290,\n",
       "        -67.7652], device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_loss(model_gen, choice, reject):\n",
    "    b = choice['input_ids'].shape[0]\n",
    "\n",
    "    #合并两部分输入,同时计算以提高效率\n",
    "    #[8, 21]\n",
    "    input_ids = torch.cat([choice['input_ids'], reject['input_ids']], dim=0)\n",
    "    attention_mask = torch.cat(\n",
    "        [choice['attention_mask'], reject['attention_mask']], dim=0)\n",
    "    label = torch.cat([choice['label'], reject['label']], dim=0)\n",
    "\n",
    "    #[8, 21, 28]\n",
    "    out = model_gen(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    #偏移以对齐\n",
    "    #[8, 20]\n",
    "    label = label[:, 1:]\n",
    "    #[8, 20, 28]\n",
    "    out = out[:, :-1]\n",
    "\n",
    "    #取所有字的预测概率,因为要求联合概率,所以取对数\n",
    "    out = (out.softmax(2) + 1e-8).log()\n",
    "\n",
    "    #索引不能是负数,所以这里把负数置0\n",
    "    #[8, 20, 1]\n",
    "    index = label.clone().unsqueeze(2)\n",
    "    index[index == -100] = 0\n",
    "\n",
    "    #取预测到label的概率\n",
    "    #[8, 20]\n",
    "    prob = torch.gather(out, dim=2, index=index).squeeze(2)\n",
    "\n",
    "    #只取答案部分的loss,筛选后,所有答案的概率对数求和\n",
    "    prob = (prob * (label != -100)).sum(1)\n",
    "\n",
    "    #choice和reject的预测概率求差作为loss\n",
    "    return prob[:b] - prob[b:]\n",
    "\n",
    "\n",
    "get_loss(model_gen, *get_batch_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb049323",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 S90.06=9.62--7.69E\n",
      "2000 S-91.98=-2.44*14.44+-94.44E\n",
      "4000 S-81.36=-2.22/-22.22+-9.22E\n",
      "6000 S-61.47=-1.11*15.45+-3.84E\n",
      "8000 S-67.18=-1.06*24.45+-3.05E\n",
      "10000 S-25.05=-9.16/-1.19+-1.19E\n",
      "12000 S-89.50=-1.22*19.49+-39.00E\n",
      "14000 S-4419.54=-52.85*82.55+-77.05E\n",
      "16000 S5128.30=-62.64*-74.66+-6.66E\n",
      "18000 S15.07=-72.98/-2.29+1.89E\n",
      "20000 S5.12=-1.21*-1.21+-1.22E\n",
      "22000 S33.73=-1.41*-1.44+39.33E\n",
      "24000 S-138.93=-92.22-94.23+59.23E\n",
      "26000 S53.01=-1.21*-22.44+2.19E\n",
      "28000 S-86.73=-4.31*1.34+-97.00E\n",
      "30000 S69.94=-1.11*-19.84+29.10E\n",
      "32000 S88.63=-1.44*-3.44+70.14E\n",
      "34000 S48.71=-1.47*-1.47+67.47E\n",
      "36000 S-30.06=-1.23*-1.87+-37.02E\n",
      "38000 S-2084.97=-32.66*66.66+-67.17E\n",
      "40000 S65.84=-1.41/-41.44+66.02E\n",
      "42000 S71.95=-1.11*-36.16+-1.12E\n",
      "44000 S1.46=-1.10*-1.10+-1.10E\n",
      "46000 S298.70=12.92*22.54+-47.17E\n",
      "48000 S36.17=-1.41*-1.44+39.03E\n",
      "50000 S3181.44=-42.99*-76.99+-5.99E\n",
      "52000 S119.07=92.15--39.44+-4.00E\n",
      "54000 S87.30=-1.44*-31.44+19.48E\n",
      "56000 S-45.69=-1.21*12.49+-24.22E\n",
      "58000 S-136.53=-9.11*12.17+-20.07E\n",
      "60000 S-86.47=-1.24*42.44+-34.03E\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model_gen.parameters(),\n",
    "                             lr=1e-4,\n",
    "                             betas=(0.9, 0.999),\n",
    "                             eps=1e-8)\n",
    "\n",
    "for i in range(10_0000):\n",
    "    choice, reject = get_batch_data()\n",
    "    loss = get_loss(model_gen, choice, reject)\n",
    "    with torch.no_grad():\n",
    "        loss_ref = get_loss(model_gen_ref, choice, reject)\n",
    "\n",
    "    #logsigmoid正数归零的激活函数,有一定的平滑\n",
    "    loss = -torch.nn.functional.logsigmoid(0.1 * (loss - loss_ref)).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if i % 2000 == 0:\n",
    "        question = tokenizer.get_data(third_number=True)\n",
    "        question = question[:question.index(tokenizer.encoder['=']) + 1]\n",
    "        question = torch.LongTensor(question).unsqueeze(0).to(device)\n",
    "\n",
    "        gen = Generater(model_gen).generate(question, max_length=35)[0]\n",
    "        print(i, tokenizer.decode(gen.tolist()))\n",
    "\n",
    "model_gen.to('cpu')\n",
    "torch.save(model_gen, 'dpo.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llama]",
   "language": "python",
   "name": "conda-env-llama-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
