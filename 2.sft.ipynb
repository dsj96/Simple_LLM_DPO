{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37e51b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/llama/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%run 1.common.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f92103b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1,  5,  3,  ...,  0,  0,  0],\n",
       "         [ 1, 11,  6,  ...,  0,  0,  0],\n",
       "         [ 1, 16,  8,  ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [ 1,  5,  3,  ...,  0,  0,  0],\n",
       "         [ 1, 16,  6,  ...,  0,  0,  0],\n",
       "         [ 1, 11, 11,  ...,  0,  0,  0]]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(data):\n",
    "    #合并x,y\n",
    "    data = [torch.cat(i) for i in data]\n",
    "\n",
    "    #求最大长度\n",
    "    lens = max([len(i) for i in data])\n",
    "\n",
    "    #做个白板\n",
    "    input_ids = torch.full((len(data), lens), tokenizer.encoder['PAD'])\n",
    "\n",
    "    #往白板里黏贴数据\n",
    "    for i, d in enumerate(data):\n",
    "        input_ids[i, :len(d)] = d\n",
    "\n",
    "    attention_mask = (input_ids != tokenizer.encoder['PAD']).long()\n",
    "\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "\n",
    "loader = get_loader(f)\n",
    "\n",
    "next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0b8671a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS-76.96=-46.97-29.99EOS\n",
      "SOS-76.96=07-c7.1ccccc1c1c1c1c1c1c1c1c1111\n",
      "---------------\n",
      "SOS0.92=64.12**-0.02EOS\n",
      "SOS0.92=5c15c1c1c1111111111111111111111111\n",
      "---------------\n",
      "SOS0.26=19.84/75.84EOS\n",
      "SOS0.26=5c11111111111111111111111111111111\n",
      "---------------\n",
      "SOS-1.48=-90.08/61.04EOS\n",
      "SOS-1.48=.1c1c1c1c111111111111111111111111\n",
      "---------------\n",
      "SOS-2569.27=-14.58**2.93EOS\n",
      "SOS-2569.27=0707070707.=05c15c15c1c1c1c1c1\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def test(N=5, device='cpu'):\n",
    "    for _ in range(N):\n",
    "        x, y = tokenizer.get_data()\n",
    "        print(tokenizer.decode(x + y))\n",
    "\n",
    "        x = torch.LongTensor(x).unsqueeze(0).to(device)\n",
    "        out = generate_model.generate(x, max_length=40)[0]\n",
    "        print(tokenizer.decode(out))\n",
    "        \n",
    "        #尝试执行计算\n",
    "        try:\n",
    "            out = out[1:-1]\n",
    "            idx_eq = (out == tokenizer.encoder['=']).nonzero().item() + 1\n",
    "            out = tokenizer.decode(out[idx_eq:])\n",
    "            print(eval(out))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        print('---------------')\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb9fe446",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "SOS163.00=83.00--80.00EOS\n",
      "SOS163.00=01111111111111111111111111111111\n",
      "---------------\n",
      "SOS-0.00=-98.92**-2.56EOS\n",
      "SOS-0.00=555555555522222EOS\n",
      "555555555522222\n",
      "---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/llama/lib/python3.9/site-packages/transformers/generation/utils.py:1468: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "SOS-107.35=-61.78-45.57EOS\n",
      "SOS-107.35=-14.77--15.76EOS\n",
      "0.9900000000000002\n",
      "---------------\n",
      "SOS1.15=-95.90/-83.34EOS\n",
      "SOS1.15=-10.00/-10.0EOS\n",
      "1.0\n",
      "---------------\n",
      "1000\n",
      "SOS19.96=-5.86+25.82EOS\n",
      "SOS19.96=-7.59--10.07EOS\n",
      "2.4800000000000004\n",
      "---------------\n",
      "SOS0.15=-10.04/-69.23EOS\n",
      "SOS0.15=-2.07/-25.78EOS\n",
      "0.08029480217222652\n",
      "---------------\n",
      "1500\n",
      "SOS1024.33=-18.74*-54.66EOS\n",
      "SOS1024.33=-73.90*-20.00EOS\n",
      "1478.0\n",
      "---------------\n",
      "SOS-0.00=-54.98**-1.77EOS\n",
      "SOS-0.00=-44.99**-1.99EOS\n",
      "-0.0005132148018997137\n",
      "---------------\n",
      "2000\n",
      "SOS-1.66=-49.99--48.33EOS\n",
      "SOS-1.66=-13.13/11.02EOS\n",
      "-1.1914700544464611\n",
      "---------------\n",
      "SOS-143.60=-53.76-89.84EOS\n",
      "SOS-143.60=-93.11+-43.14EOS\n",
      "-136.25\n",
      "---------------\n",
      "2500\n",
      "SOS-7.91=90.23+-98.14EOS\n",
      "SOS-7.91=-26.04--1.00EOS\n",
      "-25.04\n",
      "---------------\n",
      "SOS-94.04=-66.21-27.83EOS\n",
      "SOS-94.04=-6.18+-40.14EOS\n",
      "-46.32\n",
      "---------------\n",
      "3000\n",
      "SOS0.00=49.28**-3.44EOS\n",
      "SOS0.00=99.90**-2.97EOS\n",
      "1.1515704178655367e-06\n",
      "---------------\n",
      "SOS-266.23=-58.92**1.37EOS\n",
      "SOS-266.23=-7.52**2.26EOS\n",
      "-95.55475861429069\n",
      "---------------\n",
      "3500\n",
      "SOS-0.00=-80.73**-1.86EOS\n",
      "SOS-0.00=-40.00**-4.40EOS\n",
      "-8.931739295455581e-08\n",
      "---------------\n",
      "SOS-0.54=-42.45/78.34EOS\n",
      "SOS-0.54=-11.08/36.41EOS\n",
      "-0.3043120021971986\n",
      "---------------\n",
      "4000\n",
      "SOS-114.21=-67.55+-46.66EOS\n",
      "SOS-114.21=-99.00-32.07EOS\n",
      "-131.07\n",
      "---------------\n",
      "SOS56.76=61.11+-4.35EOS\n",
      "SOS56.76=-1.09--87.09EOS\n",
      "86.0\n",
      "---------------\n",
      "4500\n",
      "SOS-0.49=19.01/-38.51EOS\n",
      "SOS-0.49=-12.66/41.92EOS\n",
      "-0.3020038167938931\n",
      "---------------\n",
      "SOS116.29=68.85--47.44EOS\n",
      "SOS116.29=99.29--21.11EOS\n",
      "120.4\n",
      "---------------\n",
      "5000\n",
      "SOS1.06=82.04/77.68EOS\n",
      "SOS1.06=-91.90/-72.92EOS\n",
      "1.2602852441031267\n",
      "---------------\n",
      "SOS0.00=69.21**-1.48EOS\n",
      "SOS0.00=99.92**-4.99EOS\n",
      "1.0513187195223328e-10\n",
      "---------------\n",
      "5500\n",
      "SOS-15.83=33.78+-49.61EOS\n",
      "SOS-15.83=-79.00--74.44EOS\n",
      "-4.560000000000002\n",
      "---------------\n",
      "SOS27.70=-0.36+28.06EOS\n",
      "SOS27.70=-1.44+24.44EOS\n",
      "23.0\n",
      "---------------\n",
      "6000\n",
      "SOS-2032.27=-59.51*34.15EOS\n",
      "SOS-2032.27=-40.30*43.37EOS\n",
      "-1747.8109999999997\n",
      "---------------\n",
      "SOS-1.91=-78.18/40.93EOS\n",
      "SOS-1.91=-87.39/49.33EOS\n",
      "-1.7715386174741536\n",
      "---------------\n",
      "6500\n",
      "SOS8.29=-86.05--94.34EOS\n",
      "SOS8.29=-71.88--97.94EOS\n",
      "26.060000000000002\n",
      "---------------\n",
      "SOS0.03=5.17**-2.07EOS\n",
      "SOS0.03=1.88/43.88EOS\n",
      "0.04284412032816773\n",
      "---------------\n",
      "7000\n",
      "SOS-115.52=-85.15+-30.37EOS\n",
      "SOS-115.52=-99.07+-29.00EOS\n",
      "-128.07\n",
      "---------------\n",
      "SOS0.58=64.83**-0.13EOS\n",
      "SOS0.58=-32.07/-50.07EOS\n",
      "0.640503295386459\n",
      "---------------\n",
      "7500\n",
      "SOS-27.33=20.66-47.99EOS\n",
      "SOS-27.33=-1.37-23.39EOS\n",
      "-24.76\n",
      "---------------\n",
      "SOS85.99=15.94+70.05EOS\n",
      "SOS85.99=-1.19--89.19EOS\n",
      "88.0\n",
      "---------------\n",
      "8000\n",
      "SOS-0.00=-63.23**-1.52EOS\n",
      "SOS-0.00=-66.44**-4.41EOS\n",
      "-9.185131094339022e-09\n",
      "---------------\n",
      "SOS-2224.05=-91.60*24.28EOS\n",
      "SOS-2224.05=-46.66*54.60EOS\n",
      "-2547.636\n",
      "---------------\n",
      "8500\n",
      "SOS-5936.14=-72.64*81.72EOS\n",
      "SOS-5936.14=-82.83*74.88EOS\n",
      "-6202.310399999999\n",
      "---------------\n",
      "SOS1260.76=-56.41*-22.35EOS\n",
      "SOS1260.76=-37.70*-39.78EOS\n",
      "1499.7060000000001\n",
      "---------------\n",
      "9000\n",
      "SOS-2.91=53.55/-18.40EOS\n",
      "SOS-2.91=-81.42/31.06EOS\n",
      "-2.62137797810689\n",
      "---------------\n",
      "SOS34.15=-31.96--66.11EOS\n",
      "SOS34.15=-1.44--35.56EOS\n",
      "34.120000000000005\n",
      "---------------\n",
      "9500\n",
      "SOS158.28=-48.70*-3.25EOS\n",
      "SOS158.28=99.20+50.77EOS\n",
      "149.97\n",
      "---------------\n",
      "SOS-61.08=-87.05--25.97EOS\n",
      "SOS-61.08=-1.37+-57.19EOS\n",
      "-58.559999999999995\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss(\n",
    "        ignore_index=tokenizer.encoder['PAD'])\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    for i in range(10000):\n",
    "        input_ids, attention_mask = next(iter(loader))\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        input_ids = input_ids[:, 1:].flatten()\n",
    "        out = out[:, :-1].flatten(end_dim=1)\n",
    "        loss = criterion(out, input_ids)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(i)\n",
    "            test(2, device)\n",
    "\n",
    "    model.to('cpu')\n",
    "    torch.save(model, 'sft.model')\n",
    "\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llama]",
   "language": "python",
   "name": "conda-env-llama-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
